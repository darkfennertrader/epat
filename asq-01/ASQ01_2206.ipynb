{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistics for Quantitative Trading\n",
    "<div class=\"alert alert-info\"><strong>Part I : Statistical analysis of financial market data</strong></div>\n",
    "\n",
    "#### Notebook Created on: 20 June 2020\n",
    "##### Last Update: 28 Jun 2022\n",
    "##### Version 1.6\n",
    "##### Author: Vivek Krishnamoorthy\n",
    "\n",
    "## **Agenda for today**\n",
    "- Time series modeling - A preview before the lectures (the document)\n",
    "- Transforming prices to returns\n",
    "- Visualizing the data\n",
    "- Calculating realized volatility\n",
    "- Detecting outliers\n",
    "- Verifying the stylized facts of asset returns\n",
    "- Anatomy of a time series process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I adapt and use some of the material from [Eryk Lewinson](#eryk), the [ARCH documentation](#arch), and [other sources](#others) mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'convert'></a>\n",
    "\n",
    "### A simple technique to stationarize: Transforming prices to returns\n",
    "\n",
    "Asset prices are usually non-stationary. We try to make it stationary by dealing with its returns instead. This can be either simple returns or log returns. For daily or intraday returns, the difference between simple returns and log returns is very small. As the time scale increases, this difference grows in size. Log returns are always lesser than the corresponding simple returns.\n",
    "\n",
    "We now calculate both types of returns for `GLENMARK` prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end1 = datetime.date(2022, 6, 28)\n",
    "start1 = datetime.date(2005, 7, 1)\n",
    "ticker1 = \"GLENMARK.NS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(ticker1, start=start1, end=end1, progress=False)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker1} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.head(3))\n",
    "print(df1.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "###################### IGNORE THIS CELL #####################################\n",
    "#############################################################################\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df1.to_csv(\"GLENMARK.NS.csv\")\n",
    "# df1 = pd.read_csv(\"GLENMARK.NS\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"], inplace=True)\n",
    "df1.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)\n",
    "print(df1.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['simple_returns'] = df1['adj_close'].pct_change()\n",
    "df1['log_returns'] = np.log(df1['adj_close'] / df1['adj_close'].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.head())\n",
    "print(df1.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "\n",
    "Because time series is sequential, plotting the data allows us to get an intuitive feel for how it fluctuates over time. It also helps us reason about features that could explain the variation we see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the results together\n",
    "M = 3\n",
    "N = 1\n",
    "\n",
    "fig, ax = plt.subplots(M, N, figsize=(18, 12))\n",
    "\n",
    "ax[0].plot(df1['adj_close'], color='mediumblue', label='Prices', linewidth=0.8)\n",
    "ax[0].set_title(f\"{ticker1} stock  - Adjusted close price over time\", fontsize=20)\n",
    "ax[0].set_ylabel(\"Price (INR)\", fontsize=15)\n",
    "\n",
    "ax[1].plot(df1['simple_returns'], color='olive', label='Daily simple returns')\n",
    "ax[1].set_title(f\"{ticker1} stock  - Daily simple returns\", fontsize=20)\n",
    "ax[1].set_ylabel(\"Simple returns (%)\", fontsize=15)\n",
    "\n",
    "ax[2].plot(df1['log_returns'], color='forestgreen', label='Daily log returns')\n",
    "ax[2].set_title(f\"{ticker1} stock - Daily log returns\", fontsize=20)\n",
    "ax[2].set_ylabel(\"Log returns (%)\", fontsize=15)\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    for eachyear in range(start1.year,end1.year):\n",
    "        ax[i].axvline(pd.to_datetime(str(eachyear)+'-12-31'), color='black', linestyle='--', alpha=0.4)\n",
    "\n",
    "ax[0].axhline(df1['adj_close'].mean(), label='Mean price', color='maroon', linestyle='--', alpha=0.7)\n",
    "ax[1].axhline(0, color='maroon', label='Zero line', linestyle='--', alpha=0.7)\n",
    "ax[2].axhline(0, color='maroon', label='Zero line', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Legend addition\n",
    "#ax[0].legend(['Prices', 'Mean price'], loc='best')\n",
    "#ax[1].legend(['Daily simple returns', 'Zero line'], loc='best')\n",
    "#ax[2].legend(['Daily log returns', 'Zero line'], loc='best')\n",
    "\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "ax[2].legend(loc='best')\n",
    "\n",
    "\n",
    "# To set the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- Similarity in the shapes of the simple returns and log returns. There is a difference in scale as we see from the Y-axis.\n",
    "- Compare both return values (from the `pandas DataFrame`). What do you see?\n",
    "- Plotting the price and the returns side by side allows us to observe the relationship between price and volatility in returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ***********************************************************\n",
    "## ***** EXPERIMENTAL : IGNORE THIS CELL *********************\n",
    "## Trials with cufflinks and plotly\n",
    "## ***********************************************************\n",
    "\n",
    "## Using cufflinks and plotly\n",
    "\n",
    "# import cufflinks as cf\n",
    "# from plotly.offline import iplot, init_notebook_mode\n",
    "\n",
    "# ## Config setup\n",
    "# cf.set_config_file(world_readable=True, theme='polar',\n",
    "#                    offline=True)\n",
    "\n",
    "# init_notebook_mode()\n",
    "\n",
    "## I needed to separately install chart_studio since iplot() is not displaying the chart\n",
    "## Run the following on the Anaconda prompt or terminal\n",
    "## pip install chart_studio\n",
    "## conda install -c conda-forge nodejs\n",
    "## conda update nodejs\n",
    "## jupyterlab extension list\n",
    "## conda install -c conda-forge jupyterlab-plotly-extension\n",
    "# https://plotly.com/python/getting-started/#jupyterlab-support-python-35\n",
    "# jupyter labextension install jupyterlab-plotly@4.8.2\n",
    "# jupyter labextension list\n",
    "# Still not working properly.\n",
    "\n",
    "# df1.iplot(subplots=True, shape=(3, 1), shared_xaxes=True, \n",
    "#           title='GLENMARK adjusted close prices over time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating realized volatility\n",
    "\n",
    "Realized volatility calculates the magnitude of price movements of an asset over a specified time period. It's a commonly used risk measure.\n",
    "\n",
    "We will now use the daily data to calculate monthly realized volatility (applying the [Barndorff-Nielsen & Shephard method](#bnshephard)) shown below.\n",
    "\n",
    "1. Calculate the daily log returns\n",
    "$$r_t = log (P_t) - log (P_{t-1})$$\n",
    "\n",
    "2. Calculate the monthly realized variance by summing the squared returns for the `N` trading days of that month.\n",
    "$$ \\sum_{t=1}^N {r_t}^2$$\n",
    "\n",
    "3. The monthly realized volatility is the square root of the above expression,\n",
    "\n",
    "4. We annualize the value by multiplying the monthly realized volatility by $\\sqrt{12}$.\n",
    "\n",
    "Let's implement it in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a function to calculate realized volatility\n",
    "\n",
    "def realized_vol(x):\n",
    "    return np.sqrt(np.sum(x ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the monthly realized volatility\n",
    "\n",
    "df1_real_vol = df1.groupby(pd.Grouper(freq='M'))[['log_returns']].apply(realized_vol)\n",
    "# .apply would apply the function 'realized_vol' to the column selected (which would be \n",
    "# log returns for each month).\n",
    "# Note that the function calculates the square root of the sum of squared returns for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_real_vol.rename(columns={'log_returns': 'realized_vol'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_real_vol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_real_vol.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the annualized volatility\n",
    "\n",
    "df1_real_vol['realized_vol'] = df1_real_vol['realized_vol'] * np.sqrt(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the results together\n",
    "M = 2\n",
    "N = 1\n",
    "\n",
    "fig, ax = plt.subplots(M, N, figsize=(20, 15))\n",
    "\n",
    "ax[0].plot(df1['adj_close'], color='mediumblue', linewidth=0.8, label='Prices')\n",
    "ax[0].set_title(f\"{ticker1} stock  - Price over time\", fontsize=20)\n",
    "#ax[0].set_xlabel('Time', fontsize=15)\n",
    "ax[0].set_ylabel(\"Price (INR)\", fontsize=15)\n",
    "\n",
    "ax[1].plot(df1['log_returns'], color='forestgreen', label='Log returns')\n",
    "ax[1].plot(df1_real_vol['realized_vol'], color='orangered', label='Volatility')\n",
    "ax[1].set_title(f\"{ticker1} stock - Returns and volatility over time\", fontsize=20)\n",
    "ax[1].set_ylabel(\"Returns and volatility (in % terms)\", fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    for eachyear in range(start1.year,end1.year):\n",
    "        ax[i].axvline(pd.to_datetime(str(eachyear)+'-12-31'), color='black', linestyle='--', alpha=0.4)\n",
    "\n",
    "ax[0].axhline(df1['adj_close'].mean(), label='Mean price', color='crimson', linestyle='--', alpha=0.7)\n",
    "ax[1].axhline(0, color='crimson', label='Zero line', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Legend addition\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "\n",
    "# To set the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- Extreme price changes are synonymous with large changes in daily returns.\n",
    "- Large changes in returns often coincide with spikes in volatility\n",
    "- There are some outliers in returns which can bias our interpretations\n",
    "- We use the `.resample()` method in `pandas` when we want to change the frequency of our observations (ex. moving from daily returns to weekly returns or monthly returns). However, since realized volatility calculation is a little convoluted, we created the function `realized_vol`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting outliers\n",
    "\n",
    "Compared to many other fields in the social sciences, financial market data for the most part is quite clean (with relatively fewer errors). Nevertheless, it still behooves us to analyze our data and identify any observations that are significantly different from their neighboring observations. We call such data points, **outliers**.\n",
    "\n",
    "Outliers could be because of:\n",
    "- Incorrectly entered or calculated in the data source\n",
    "- Missing data \n",
    "- Missing data auto-filled with random values\n",
    "- A major market event, etc.\n",
    "\n",
    "Outliers in our data affect both, the analysis we do and the results that follow when building a model or backtesting a strategy. No matter how good our trading strategy looks or how sophisticated our model is, remember: *'garbage in, garbage out'*.\n",
    "\n",
    "Let's look at one way of catching and marking out the outliers in the `GLENMARK` data. We define outliers as any return value that is more than three standard deviations away from the mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_length = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the one month rolling mean and standard deviation\n",
    "\n",
    "df1_rolling_stats = df1['simple_returns'].rolling(window\n",
    "                                                  =window_length).agg(['mean',\n",
    "                                                                       'std'], axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_rolling_stats.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joining the newly created data to the original data\n",
    "\n",
    "df1_all = df1.join(df1_rolling_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a function to catch outliers\n",
    "\n",
    "def catch_outliers(row, n_sigmas=3):\n",
    "    x = row['simple_returns']\n",
    "    μ = row['mean']\n",
    "    σ = row['std']\n",
    "    \n",
    "    if (x > μ + 3 * σ) | (x < μ - 3 * σ):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Catch the outliers\n",
    "\n",
    "df1_all['outlier'] = df1_all.apply(catch_outliers, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract their values\n",
    "\n",
    "outliers = df1_all.loc[df1_all['outlier'] == 1, ['simple_returns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the results together\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax.plot(df1_all.index, df1_all['simple_returns'], color='royalblue', \n",
    "        linewidth=0.8, label=\"Normal\")\n",
    "ax.scatter(outliers.index, outliers['simple_returns'], color='red',\n",
    "           label=\"Outlier\")\n",
    "ax.set(title=f\"{ticker1} daily stock returns\", \n",
    "          ylabel=\"Simple returns (%)\")\n",
    "\n",
    "for eachyear in range(start1.year,end1.year):\n",
    "    ax.axvline(pd.to_datetime(str(eachyear)+'-12-31'), color='black', linestyle='--', alpha=0.4)\n",
    "\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- Outliers are marked with red dots. However, note that when there are multiple large returns (in magnitude) occurring in the same period, only the first gets identified and marked.\n",
    "- This is due to the increase in the the values of the rolling statistics when multiple adjacent outliers enter the window.\n",
    "- We can vary the window length based on the underlying data frequency. Here, the moving averages calculated would give monthly means and standard deviations of returns.\n",
    "- We can also use exponential moving averages (EMA) instead of simple moving averages if we want the rolling statistics to react faster to recent price changes.\n",
    "- Here, we have only identified and tagged them. The next important step is how to deal with them (which is outside the scope of this session). I leave you to explore this topic as a self-study exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stylized facts of asset returns\n",
    "\n",
    "[Cont (2001)](#cont2001) does a detailed review of the empirical features of asset prices in financial markets. He presents common statistical properties (***stylized facts***) that financial asset returns share across markets, instruments and time periods.\n",
    "\n",
    "We should be cognizant of them when we develop models to explain and forecast asset prices. \n",
    "We now work with 40 years of S&P daily data to examine some of their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as scs\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end2 = datetime.date(2022, 6, 27)\n",
    "start2 = datetime.date(1980, 1, 2)\n",
    "ticker2 = \"^GSPC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(ticker2, start=start2, end=end2, progress=False)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker2} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "###################### IGNORE THIS CELL #####################################\n",
    "#############################################################################\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df2.to_csv(\"GLENMARK.NS.csv\")\n",
    "# df2 = pd.read_csv(\"GLENMARK.NS\", index_col=0, parse_dates=True)\n",
    "# df2 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.head())\n",
    "print(df2.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"], inplace=True)\n",
    "df2.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)\n",
    "print(df2.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['log_returns'] = np.log(df2['adj_close'] / df2['adj_close'].shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Non-normal distribution of returns\n",
    "\n",
    "A standard assumption in finance models (like the CAPM, the Black-Scholes option pricing model) is that returns are normally distributed.\n",
    "\n",
    "Numerous studies have however shown empirically that this is not quite true.\n",
    "Specifically,\n",
    "- *Left skewed* : Gains and loss distributions are not symmetrical. \n",
    "    - We see longer left tails compared to the right tails\n",
    "    - The large negative returns are higher in magnitude compared to the large positive returns. \n",
    " \n",
    "- *Excess kurtosis* : The distribution is fat at the tails and higher than normal peaks\n",
    "    - Large (and small) returns occur more frequently than the distributional assumption of normality expects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dropna(axis='rows', how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating a normal distribution curve with mean, variance, and range obtained from S&P 500 returns data\n",
    "\n",
    "log_returns_range = np.linspace(min(df2['log_returns']), max(df2['log_returns']), num=1000)\n",
    "μ = df2['log_returns'].mean()\n",
    "σ = df2['log_returns'].std()\n",
    "norm_pdf = scs.norm.pdf(log_returns_range, loc=μ, scale=σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1\n",
    "N = 2\n",
    "\n",
    "fig, ax = plt.subplots(M, N, figsize=(15, 7.5))\n",
    "\n",
    "## histogram\n",
    "\n",
    "sns.distplot(df2['log_returns'], kde=False, norm_hist=True, ax=ax[0])\n",
    "ax[0].set_title(\"Distribution of S&P 500 returns\", fontsize=15)\n",
    "ax[0].plot(log_returns_range, norm_pdf, color='green', \n",
    "           linewidth=2, label=f\"N({μ:.4f}, {σ**2:.5f})\")\n",
    "ax[0].legend(loc=\"best\")\n",
    "\n",
    "## Q-Q plot\n",
    "\n",
    "qq_plot = sm.qqplot(df2['log_returns'], line='s', ax=ax[1])\n",
    "## 's' is for standardized line to compare the plot with a normal distribution\n",
    "ax[1].set_title(\"Q-Q plot\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note: *Distribution of S&P 500 returns*\n",
    "- There is a visible difference in the shape of the returns histogram and the Gaussian (normal) distribution curve.\n",
    "- The peak is higher in the histogram than the normal curve.\n",
    "- The left tail of the distribution is longer.\n",
    "\n",
    "Points to note: *Q-Q plot*\n",
    "\n",
    "- Q-Q plots are used to compare empirical data to theoretical distributions. They are helpful in finding deviations at the tails.\n",
    "- Here, we compare the distribution of the observed returns to a normal distribution.\n",
    "- If we find that the dots are more or less on the red line, then it means that the data (in this case returns) is normally distributed.\n",
    "- There is drift at both ends of the tails. This means that we have fatter tails violating the normality assumption.\n",
    "- The size of the drift is higher on the left side of the plot than on the right side. This means that we observe large drops in returns but not equally large growths in returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the Jarque-Bera test to check for normality of the sample data\n",
    "## If the test statistic is very large and the p-value is less than 5%, we infer that the data is not\n",
    "## normally distributed.\n",
    "\n",
    "## For more details, check https://en.wikipedia.org/wiki/Jarque-Bera_test\n",
    "\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "jb_test_stat, pvalue, _, _ = jarque_bera(df2['log_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarque_bera?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the summmary statistics of the S&P 500 returns data\n",
    "\n",
    "print('------------------- Summary Statistics --------------------')\n",
    "print(f'Range of dates: {min(df2.index.date)} to {max(df2.index.date)}')\n",
    "print(f'Number of observations: {df2.shape[0]}')\n",
    "print(f\"Mean: {df2['log_returns'].mean():.4f}\")\n",
    "print(f\"Median: {df2['log_returns'].median():.4f}\")\n",
    "print(f\"Min: {df2['log_returns'].min():.4f}\")\n",
    "print(f\"Max: {df2['log_returns'].max():.4f}\")\n",
    "print(f\"Standard Deviation: {df2['log_returns'].std():.4f}\")\n",
    "print(f\"Skewness: {df2['log_returns'].skew():.4f}\")\n",
    "print(f\"Kurtosis: {df2['log_returns'].kurtosis():.4f}\") \n",
    "print(f\"Jarque-Bera statistic: {jb_test_stat:.2f} with p-value: {pvalue:.2f}\")\n",
    "print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The mean is less than the median. Seen in negatively skewed distributions.\n",
    "- Confirmed by the coefficient of skewness (which is negative).\n",
    "- Excess kurtosis seen (anything above 0 is deemed as excess kurtosis. Normal distribution has a kurtosis of 0 when measured using the `pandas kurtosis` method) .\n",
    "- The p-value of the Jarque-Bera test shows non-normality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Volatility Clustering\n",
    "\n",
    "> *When it rains, it pours*.\n",
    "\n",
    "We observe from historical data that the volatility of the returns appears to change over time. Relatively tranquil periods in the market persist for a while. Similarly market turbulence like in 2008 and 2009 also cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating normally distributed returns for GLENMARK and S&P 500\n",
    "sample1 = pd.DataFrame(np.random.normal(loc=df1['log_returns'].mean(), \n",
    "                                     scale=df1['log_returns'].std(), \n",
    "                                     size=df1['log_returns'].shape[0]), \n",
    "                    index = df1.index)\n",
    "\n",
    "sample2 = pd.Series(np.random.normal(loc=df2['log_returns'].mean(), \n",
    "                                     scale=df2['log_returns'].std(), \n",
    "                                     size=df2['log_returns'].shape[0]), \n",
    "                    index = df2.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1.loc['July 2005', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the daily returns of GLENMARK and simulated data with same mean/sd side by side \n",
    "\n",
    "x = df1.index\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 9), sharey=True)\n",
    "ax1.plot(x, df1['log_returns'], linewidth=0.8, color='mediumblue')\n",
    "ax1.set_title(\"GLENMARK returns\", fontsize=15)\n",
    "ax2.plot(sample1, linewidth=0.8, color='crimson')\n",
    "ax2.set_title(\"Simulated returns\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the daily returns of S&P 500 and simulated data with same mean/sd side by side \n",
    "\n",
    "x = df2.index\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 9), sharey=True)\n",
    "ax1.plot(x, df2['log_returns'], linewidth=0.8, color='mediumblue')\n",
    "ax1.set_title(\"S&P 500 returns\", fontsize=15)\n",
    "ax2.plot(x, sample2, linewidth=0.8, color='maroon')\n",
    "ax2.set_title(\"Simulated returns\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the daily returns of GLENMARK and S&P 500 side by side \n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 9))\n",
    "\n",
    "ax[0].set_title(\"Distribution of GLENMARK returns\", fontsize=15)\n",
    "ax[0].plot(df1['log_returns'], linewidth=0.8, color='mediumblue')\n",
    "\n",
    "ax[1].set_title(\"Distribution of S&P 500 returns\", fontsize=15)\n",
    "ax[1].plot(df2['log_returns'], linewidth=0.8, color='olive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- For Glenmark, between 2008 and 2010, there's a higher swing of positive and negative returns. Notice how it's quite different from between 2016 and 2018.\n",
    "- Similar waves of high and low volatility periods seen in S&P returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Absence of autocorrelations\n",
    "\n",
    "Autocorrelation measures the degree of similarity between a time series process and a delayed copy of itself. We measure it at different lags to unearth any patterns that repeat consistently.\n",
    "\n",
    "Studies have shown that there is little autocorrelation observed in asset returns except for short time scales (of the order of 15 to 20 minutes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = 50\n",
    "significance_level = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "N = 5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(M, N))\n",
    "smt.graphics.plot_acf(df2['log_returns'], lags=n_lags, \n",
    "                                 alpha=significance_level, ax=ax)\n",
    "ax.set_title(\"Autocorrelation plot of S&P 500 daily returns\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- We do not look at autocorrelation at lag 0.\n",
    "- A few values that lie outside the blue confidence interval are statistically significant\n",
    "- Visually, it appears that there is very little autocorrelation in the returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "N = 5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(M, N))\n",
    "smt.graphics.plot_acf(df2['adj_close'], lags=n_lags, \n",
    "                                 alpha=significance_level, ax=ax)\n",
    "ax.set_title(\"Autocorrelation plot of the S&P 500\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Slow decay of autocorrelations in absolute and squared returns\n",
    "\n",
    "We follow the same procedure as above but this time we run it on absolute returns and squared returns. We can then plot the autocorrelation function (ACF) at different lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N = 10, 7\n",
    "fig, ax = plt.subplots(2, 1, figsize=(M, N))\n",
    "smt.graphics.plot_acf(df2['log_returns'] ** 2, lags=n_lags, \n",
    "                      alpha=significance_level, ax=ax[0])\n",
    "ax[0].set_title(f\"Autocorrelation plots of {ticker2}\", fontsize=15)\n",
    "ax[0].set(ylabel=\"Squared Returns\")\n",
    "\n",
    "smt.graphics.plot_acf(np.abs(df2['log_returns']), lags=n_lags, \n",
    "                      alpha=significance_level, ax=ax[1])\n",
    "ax[1].set(title=\"\", xlabel='Lags', ylabel=\"Absolute Returns\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- There is is a slow and uneven decay of the ACF plot.\n",
    "- The ACF of the squared returns decrease faster than that of the absolute returns.\n",
    "- There is significant autocorrelation as seen in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Leverage effect\n",
    "\n",
    "There is a negative correlation between the volatility (most measures) of an asset and its returns. i.e. When prices go up, there is less volatility in the asset returns and vice-versa.\n",
    "\n",
    "We verify it using two different methods. \n",
    "\n",
    "- In the first one, we measure volatility as the standard deviation of the asset returns. \n",
    "- In the second one, we will use the VIX index (often called the *fear index* of Wall Street), a popular market metric that tracks expectations of volatility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['ma_std_252'] = df1['log_returns'].rolling(window=252).std()\n",
    "df1['ma_std_21'] = df1['log_returns'].rolling(window=21).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method I\n",
    "\n",
    "M, N = 16, 12\n",
    "fig, ax = plt.subplots(3, 1, figsize=(M, N))\n",
    "\n",
    "df1['adj_close'].plot(ax=ax[0], linewidth=0.8, color='mediumblue')\n",
    "ax[0].set_title(f\"{ticker1} time series \", fontsize=16)\n",
    "ax[0].set(ylabel=\"Adjusted Close Price (INR)\")\n",
    "\n",
    "\n",
    "df1['log_returns'].plot(ax=ax[1], linewidth=0.8, color='olive')\n",
    "ax[1].set(ylabel=\"Daily log returns (%)\")\n",
    "\n",
    "df1['ma_std_21'].plot(ax=ax[2], color='forestgreen', linewidth = 0.9, label=\"21-day MA of volatility\")\n",
    "df1['ma_std_252'].plot(ax=ax[2], color='red', linewidth = 0.9, label=\"252-day MA of volatility\")\n",
    "ax[2].set(xlabel=\"Date\", ylabel=\"Moving volatility\")\n",
    "ax[2].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- Leverage effect is visible. There seems to be a pattern of prices going up and volatility being low in those phases and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method II\n",
    "\n",
    "start3 = start2\n",
    "end3 = end2\n",
    "ticker3 = \"^VIX\"\n",
    "\n",
    "df = yf.download([ticker2, ticker3], start=start3, end=end3, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[['Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns = df3.columns.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.rename(columns={\"^GSPC\": \"sp500\", \"^VIX\": \"vix\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"log_returns\"] = np.log(df3['sp500'] / df3['sp500'].shift(1))\n",
    "df3[\"vol_returns\"] = np.log(df3['vix'] / df3['vix'].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.dropna(how=\"any\", axis=\"rows\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coeff = df3['log_returns'].corr(df3['vol_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.set(font_scale=2)\n",
    "ax = sns.regplot(x='log_returns', y='vol_returns', data=df3, \n",
    "                 line_kws={'color':'red'})\n",
    "ax.set(title=f\"S&P 500 vs. VIX (ρ = {corr_coeff:.3f})\", \n",
    "       xlabel=\"S&P 500 log returns\", ylabel=\"VIX log returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- High negative value of correlation coefficient.\n",
    "- Slope of the regression line which is also negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'convert'></a>\n",
    "\n",
    "### Anatomy of a time series process\n",
    "\n",
    "Time series processes have a wide variety of patterns. So it is helpful to consider them as a combination of *systematic* and *unsystematic* components.\n",
    "\n",
    "- **Systematic**: These are recurring in nature and so can be described and modeled.\n",
    "- **Non-systematic**: These are random in nature and so cannot be directly modeled.\n",
    "\n",
    "The systematic components can be further split into *level*, *trend*, and *seasonality* whereas the non-systematic component is referred to as *noise*.\n",
    "\n",
    "- **Level**: The average value of the process.\n",
    "- **Trend**: The direction and rate of change of the process. The slope is a good proxy for it.\n",
    "- **Seasonality**: Deviations in the process caused by recurring short-term cycles.\n",
    "- **Noise**: The random variation observed in the process.\n",
    "\n",
    "Another useful abstraction while analyzing time series processes is to see them as either an *additive* or a *multiplicative* blend of the four constituent parts mentioned.\n",
    "\n",
    "**Additive model**: The process $X(t)$ has the form\n",
    "$$X(t) = Level + Trend + Seasonality + Noise$$\n",
    "\n",
    "We use an additive model when the underlying process under examination has the following characteristics.\n",
    "- The process changes remain constant over time (i.e. they are linear). So the trend line would be straight.\n",
    "- It shows linear seasonality. That is to say the frequency and amplitude (i.e. the width and the height) of the cycles remain constant over time.\n",
    "\n",
    "\n",
    "\n",
    "**Multiplicative model**: The process $X(t)$ has the form\n",
    "$$X(t) = Level \\times Trend \\times Seasonality \\times Noise$$\n",
    "\n",
    "We use a multiplicative model when the underlying process under examination has the following characteristics.\n",
    "- The process changes vary over time (i.e. they are non-linear in nature).\n",
    "- An exponential or quadratic or higher order polynomial process is multiplicative. So the trend-line would be curved, not straight. \n",
    "- It shows non-linear seasonality. That is to say the frequency and amplitude (i.e. the width and the height) of the cycles vary over time.\n",
    "\n",
    "In the (harsh) real world, we often encounter non-linear or even mixed processes and therefore have to work with the multiplicative model as our prototype. But we prefer to work with linear processes as they are easier to use. So we transform the original process into a linear one. A commonly used trick is applying a log transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'references'></a>\n",
    "#### References\n",
    "<a id = 'bnshephard'></a>\n",
    "<a id = 'arch'></a>\n",
    "<a id = 'others'></a>\n",
    "<a id = 'eryk'></a>\n",
    "<a id = 'cont2001'></a>\n",
    "\n",
    "1. Barndorff‐Nielsen, O. E., & Shephard, N. (2002). Econometric analysis of realized volatility and its use in estimating stochastic volatility models. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 64(2), 253-280.\n",
    "2. Cochrane, John H. \"Time series for macroeconomics and finance.\" Manuscript, University of Chicago (2005).\n",
    "3. Cont, R.(2001). Empirical properties of asset returns: stylized facts and statistical issues.\n",
    "4. https://towardsdatascience.com/@eryk.lewinson\n",
    "5. https://pyflux.readthedocs.io/en/latest/getting_started.html\n",
    "6. https://tomaugspurger.github.io/modern-7-timeseries\n",
    "7. https://arch.readthedocs.io/en/latest/univariate/univariate_volatility_modeling.html\n",
    "6. Tsay, Ruey S. Analysis of financial time series. Vol. 543. John Wiley & Sons, 2005.\n",
    "7. Campbell, John Y., Andrew Wen-Chuan Lo, and Craig MacKinlay. The Econometrics of Financial Markets. Vol. 2. Princeton, NJ: princeton University press, 1997."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
